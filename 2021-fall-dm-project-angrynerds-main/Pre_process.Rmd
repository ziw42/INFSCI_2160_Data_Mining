---
title: "Pre_process"
author: "Zian_Wang"
date: "2021/11/26"
output: html_document
---

```{r document_setup, echo=F, message=F, warning=F}
# load libraries
library('foreign') ## for loading dta files using read.dta
library('ggplot2')
library("plyr") # for recoding data
library("stats")
library("cluster") ## for h-clustring
library("dplyr") ## for making the data frames
library("knitr") ## for make the table in 2-4

theme_set(theme_bw()) # change the default ggplot theme to black-and-white

knitr::opts_chunk$set(
  echo=T, ## show your R code chunk
  message = F, ## hide the message
  warning = F, ## hide the warning
  autodep = T ## make sure your separate code chunks can find the dependencies (from other code chunk)

)
```

Read file here:

```{r, warning=F}
Sys.setlocale(category = "LC_ALL", locale = "us")
readFile <- function(filename, num) {
  a <- read.csv("./data.csv", header = T)
  a <- a[c("section", "category")]
  text <- c()
  labels <- c()
  con <- file(filename, "r")
  line = readLines(con, n=1)
  while(length(line) != 0) {
    
    line <- gsub("\\t", "", line)
    line <- gsub("\\n", "", line)
    if(nchar(line) >= 60) {
      text <- c(text, line)
    }
    line <- readLines(con, n=1)
  }
  return(text)
}
text <- c()
for(i in list.files("./data")) {
  text <- c(text, readFile(paste("./data/", i, sep = "")))
}
write.csv(text, "./temp_data.csv", row.names = FALSE, col.names = FALSE)
```

Ignore this one!!!

```{python}
import sys
sys.path.append('../')
sys.path
from 

# initialize data
labeled_documents = [("The dog is running on the field and chasing a cat"*10, ["animal"]),
                     ("apple is a good fruit and orange is full of vitamin C"*10, ["fruit"]),
                     ("Ford is popular in the U.S., also the chevelot and other car"*10, ["vehicle"]),
                     ("Your performance is really good, good job!"*10, ["positive"]),
                     ("You cannot cheat on others, shame on you."*10, ["negative"])]

# new a Labeled LDA model
# llda_model = llda.LldaModel(labeled_documents=labeled_documents, alpha_vector="50_div_K", eta_vector=0.001)
# llda_model = llda.LldaModel(labeled_documents=labeled_documents, alpha_vector=0.02, eta_vector=0.002)
llda_model = llda.LldaModel(labeled_documents=labeled_documents, alpha_vector=0.01)
print(llda_model)

# training
# llda_model.training(iteration=10, log=True)
while True:
    print("iteration %s sampling..." % (llda_model.iteration + 1))
    llda_model.training(1)
    print("after iteration: %s, perplexity: %s" % (llda_model.iteration, llda_model.perplexity()))
    print("delta beta: %s" % llda_model.delta_beta)
    if llda_model.is_convergent(method="beta", delta=0.01):
        break

# update
print("before updating: ", llda_model)
update_labeled_documents = [("dark souls 3 is good", ["game"])]
llda_model.update(labeled_documents=update_labeled_documents)
print("after updating: ", llda_model)

# train again
# llda_model.training(iteration=10, log=True)
while True:
    print("iteration %s sampling..." % (llda_model.iteration + 1))
    llda_model.training(1)
    print("after iteration: %s, perplexity: %s" % (llda_model.iteration, llda_model.perplexity()))
    print("delta beta: %s" % llda_model.delta_beta)
    if llda_model.is_convergent(method="beta", delta=0.01):
        break

# inference
# note: the result topics may be different for difference training, because gibbs sampling is a random algorithm
document = "dog catching cat" * 100

topics = llda_model.inference(document=document, iteration=100, times=10)
print(topics)
sum = 0
for t in topics:
   sum = sum + t[1]
print("t = " + str(sum))

# perplexity
# calculate perplexity on test data
perplexity = llda_model.perplexity(documents=["example example example example example",
                                              "test llda model test llda model test llda model",
                                              "example test example test example test example test",
                                              "good perfect good good perfect good good perfect good",
                                              "bad bad down down bad bad down"],
                                   iteration=30,
                                   times=10)
print("perplexity on test data: %s" % perplexity)
# calculate perplexity on training data
print("perplexity on training data: %s" % llda_model.perplexity())

# save to disk
save_model_dir = "../data/model"
# llda_model.save_model_to_dir(save_model_dir, save_derivative_properties=True)
llda_model.save_model_to_dir(save_model_dir)

# load from disk
llda_model_new = llda.LldaModel()
llda_model_new.load_model_from_dir(save_model_dir, load_derivative_properties=False)
print("llda_model_new", llda_model_new)
print("llda_model", llda_model)
print("Top-5 terms of topic 'negative': ", llda_model.top_terms_of_topic("negative", 5, False))
print("Doc-Topic Matrix: \n", llda_model.theta)
print("Topic-Term Matrix: \n", llda_model.beta)

```

Do not use the following trunk!!!

```{r pressure, echo=FALSE, warning=FALSE}
Sys.setlocale(category = "LC_ALL", locale = "us")
readFile <- function(filename, start, end) {
  a <- read.csv("./data.csv", header = T)
  a <- a[c("section", "category")]
  a <- a[start:end,]
  length <- nrow(a)
  text <- c()
  labels <- c()
  con <- file(filename, "r")
  line <- readLines(con, n=1)
  while(length(line) != 0) {
    line <- gsub("\\t", "", line)
    line <- gsub("\\n", "", line)
    if(nchar(line) >= 60) {
      text <- c(text, line)
      i <- 1
      sign <- TRUE
      while(i <= length && sign) {
        if(a[i,"section"] == substring(line, 0, nchar(a[i,"section"]))) {
           sign <- FALSE
        }
        i <- i+1
      }
      if(!sign) {
        labels <- c(labels, a[i-1,"section"])
      }
      else {
        labels <- c(labels, "Not problematic")
      }
    }
    line <- readLines(con, n=1)
  }
  text <- cbind(as.data.frame(text), as.data.frame(labels))
  return(text)
}
text <- c()
for(i in list.files("./data")) {
  text <- c(text, readFile(paste("./data/", i, sep = ""), 1, 10))
  break
}
```
